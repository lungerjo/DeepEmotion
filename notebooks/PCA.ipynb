{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. Using GPU: Quadro M4000\n",
      "{'project_root': '/home/paperspace/DeepEmotion', 'data': {'data_path': '${project_root}/data/raw/derivatives/non-linear_anatomical_alignment', 'zarr_path': '${project_root}/dataset.zarr', 'label_path': '${project_root}/data/resampled_annotations/av1o6_resampled.tsv', 'sessions': ['01', '02', '03', '04', '05', '06', '07', '08'], 'file_pattern_template': '*_ses-forrestgump_task-forrestgump_rec-dico7Tad2grpbold7TadNL_run-{}_bold.nii.gz', 'subjects': ['sub-01', 'sub-02', 'sub-03', 'sub-04', 'sub-05', 'sub-06', 'sub-07', 'sub-08', 'sub-09', 'sub-11', 'sub-12', 'sub-13', 'sub-14', 'sub-15', 'sub-16', 'sub-17', 'sub-18', 'sub-19', 'sub-20'], 'session_offsets': [0, 902, 1784, 2660, 3636, 4560, 5438, 6522], 'emotion_idx': {'NONE': 0, 'HAPPINESS': 1, 'FEAR': 2, 'SADNESS': 3, 'LOVE': 4, 'ANGERRAGE': 5, 'CONTEMPT': 6, 'GRATITUDE': 7, 'ADMIRATION': 8, 'COMPASSION': 9, 'PRIDE': 10, 'REMORSE': 11, 'DISAPPOINTMENT': 12, 'HAPPYFOR': 13, 'GLOATING': 14, 'SATISFACTION': 15, 'HOPE': 16, 'HATE': 17, 'RELIEF': 18, 'SHAME': 19, 'GRATIFICATION': 20, 'FEARSCONFIRMED': 21}, 'normalization': False, 'weight_decay': 0}, 'train': {'epochs': 50, 'batch_size': 25, 'shuffle': True, 'train_ratio': 0.8}, 'verbose': True, 'wandb': False}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.dataset import get_data_loaders\n",
    "import hydra\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from hydra import initialize, compose\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"CUDA is available. Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA is NOT available. Using CPU.\")\n",
    "\n",
    "hydra.core.global_hydra.GlobalHydra.instance().clear()\n",
    "with initialize(version_base=None, config_path=\"../src/configs\"):\n",
    "    cfg = compose(config_name=\"base\", overrides=[\"project_root=/home/paperspace/DeepEmotion\"])\n",
    "print(cfg)\n",
    "\n",
    "cfg.train.batch_size = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset contains 152 files.\n",
      "Spatial dimensions: (132, 175, 48)\n",
      "Maximum timepoints per file: 542\n",
      "Subjects: ['sub-01' 'sub-02' 'sub-03' 'sub-04' 'sub-05' 'sub-06' 'sub-07' 'sub-08'\n",
      " 'sub-09' 'sub-11' 'sub-12' 'sub-13' 'sub-14' 'sub-15' 'sub-16' 'sub-17'\n",
      " 'sub-18' 'sub-19' 'sub-20']\n",
      "Sessions: ['01' '02' '03' '04' '05' '06' '07' '08']\n",
      "Emotion categories: ['NONE', 'HAPPINESS', 'FEAR', 'SADNESS', 'LOVE', 'ANGERRAGE', 'CONTEMPT', 'GRATITUDE', 'ADMIRATION', 'COMPASSION', 'PRIDE', 'REMORSE', 'DISAPPOINTMENT', 'HAPPYFOR', 'GLOATING', 'SATISFACTION', 'HOPE', 'HATE', 'RELIEF', 'SHAME', 'GRATIFICATION', 'FEARSCONFIRMED']\n",
      "Total valid labeled timepoints: 28424\n",
      "Cell 2 complete: Loaded the dataset.\n",
      "Number of train batches: 23\n",
      "Shape of one batch of data: torch.Size([1024, 132, 175, 48])\n"
     ]
    }
   ],
   "source": [
    "train_dataloader, val_dataloader = get_data_loaders(cfg)\n",
    "\n",
    "num_train_batches = len(train_dataloader)\n",
    "sample_batch = next(iter(train_dataloader))\n",
    "batch_data = sample_batch[\"data_tensor\"]\n",
    "\n",
    "print(f\"Cell 2 complete: Loaded the dataset.\")\n",
    "print(f\"Number of train batches: {num_train_batches}\")\n",
    "print(f\"Shape of one batch of data: {batch_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "Running Incremental PCA on cuda...\n",
      "Processing batch 0\n",
      "Batch 0 GPU memory usage:\n",
      "  Before processing: allocated=235.25 MB, reserved=486.54 MB\n",
      "  After processing: allocated=1375.21 MB, reserved=1625.29 MB\n",
      "Processing batch 1\n",
      "Batch 1 GPU memory usage:\n",
      "  Before processing: allocated=235.25 MB, reserved=264.24 MB\n",
      "  After processing: allocated=1375.21 MB, reserved=1625.29 MB\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m num_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# Total number of processed samples\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Process data batch by batch\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mProcessing batch \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mbatch_idx\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallocated_before\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmemory_allocated\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e6\u001b[39;49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deepemotion-r8YRC923-py3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deepemotion-r8YRC923-py3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deepemotion-r8YRC923-py3.12/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:50\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deepemotion-r8YRC923-py3.12/lib/python3.12/site-packages/torch/utils/data/dataset.py:420\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/DeepEmotion/src/utils/dataset.py:67\u001b[0m, in \u001b[0;36mZarrDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     64\u001b[0m volume_idx, row_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalid_indices[idx]\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Extract the data slice and corresponding label\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m data_slice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvolume_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     68\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[volume_idx, row_idx]\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# Validate subject/session mapping\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deepemotion-r8YRC923-py3.12/lib/python3.12/site-packages/zarr/core.py:796\u001b[0m, in \u001b[0;36mArray.__getitem__\u001b[0;34m(self, selection)\u001b[0m\n\u001b[1;32m    794\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvindex[selection]\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_pure_orthogonal_indexing(pure_selection, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim):\n\u001b[0;32m--> 796\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_orthogonal_selection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpure_selection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfields\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfields\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    798\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_basic_selection(pure_selection, fields\u001b[38;5;241m=\u001b[39mfields)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deepemotion-r8YRC923-py3.12/lib/python3.12/site-packages/zarr/core.py:1078\u001b[0m, in \u001b[0;36mArray.get_orthogonal_selection\u001b[0;34m(self, selection, out, fields)\u001b[0m\n\u001b[1;32m   1075\u001b[0m \u001b[38;5;66;03m# setup indexer\u001b[39;00m\n\u001b[1;32m   1076\u001b[0m indexer \u001b[38;5;241m=\u001b[39m OrthogonalIndexer(selection, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m-> 1078\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_selection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfields\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfields\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deepemotion-r8YRC923-py3.12/lib/python3.12/site-packages/zarr/core.py:1341\u001b[0m, in \u001b[0;36mArray._get_selection\u001b[0;34m(self, indexer, out, fields)\u001b[0m\n\u001b[1;32m   1338\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m math\u001b[38;5;241m.\u001b[39mprod(out_shape) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1339\u001b[0m     \u001b[38;5;66;03m# allow storage to get multiple items at once\u001b[39;00m\n\u001b[1;32m   1340\u001b[0m     lchunk_coords, lchunk_selection, lout_selection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mindexer)\n\u001b[0;32m-> 1341\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_chunk_getitems\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlchunk_coords\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlchunk_selection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1345\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlout_selection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1346\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdrop_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindexer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1347\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfields\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfields\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1348\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1349\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out\u001b[38;5;241m.\u001b[39mshape:\n\u001b[1;32m   1350\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deepemotion-r8YRC923-py3.12/lib/python3.12/site-packages/zarr/core.py:2182\u001b[0m, in \u001b[0;36mArray._chunk_getitems\u001b[0;34m(self, lchunk_coords, lchunk_selection, out, lout_selection, drop_axes, fields)\u001b[0m\n\u001b[1;32m   2180\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_meta_array, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m   2181\u001b[0m         contexts \u001b[38;5;241m=\u001b[39m ConstantMap(ckeys, constant\u001b[38;5;241m=\u001b[39mContext(meta_array\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_meta_array))\n\u001b[0;32m-> 2182\u001b[0m     cdatas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_store\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43mckeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2184\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ckey, chunk_select, out_select \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(ckeys, lchunk_selection, lout_selection):\n\u001b[1;32m   2185\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ckey \u001b[38;5;129;01min\u001b[39;00m cdatas:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deepemotion-r8YRC923-py3.12/lib/python3.12/site-packages/zarr/_storage/store.py:179\u001b[0m, in \u001b[0;36mBaseStore.getitems\u001b[0;34m(self, keys, contexts)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgetitems\u001b[39m(\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28mself\u001b[39m, keys: Sequence[\u001b[38;5;28mstr\u001b[39m], \u001b[38;5;241m*\u001b[39m, contexts: Mapping[\u001b[38;5;28mstr\u001b[39m, Context]\n\u001b[1;32m    156\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Mapping[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[1;32m    157\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Retrieve data from multiple keys.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \n\u001b[1;32m    159\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;124;03m    keys and/or to utilize the contexts.\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {k: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m keys \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m}\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deepemotion-r8YRC923-py3.12/lib/python3.12/site-packages/zarr/storage.py:1118\u001b[0m, in \u001b[0;36mDirectoryStore.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1116\u001b[0m filepath \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath, key)\n\u001b[1;32m   1117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(filepath):\n\u001b[0;32m-> 1118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fromfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1119\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1120\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deepemotion-r8YRC923-py3.12/lib/python3.12/site-packages/zarr/storage.py:1093\u001b[0m, in \u001b[0;36mDirectoryStore._fromfile\u001b[0;34m(fn)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Read data from a file\u001b[39;00m\n\u001b[1;32m   1081\u001b[0m \n\u001b[1;32m   1082\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1090\u001b[0m \u001b[38;5;124;03mfile reading logic.\u001b[39;00m\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1092\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(fn, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m-> 1093\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define PCA parameters\n",
    "n_components = 50  # Set desired number of principal components\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"device: {device}\")\n",
    "\n",
    "# Get dataset dimensions from a single batch\n",
    "sample_batch = next(iter(train_dataloader))[\"data_tensor\"]\n",
    "num_features = sample_batch.shape[1] * sample_batch.shape[2] * sample_batch.shape[3]  # 132 * 175 * 48\n",
    "\n",
    "# Initialize principal components randomly\n",
    "W = torch.randn(num_features, n_components, device=device)  # Shape: (num_features, n_components)\n",
    "mean_running = torch.zeros(num_features, device=device)  # Running mean\n",
    "num_samples = 0  # Total number of processed samples\n",
    "\n",
    "# Process data batch by batch\n",
    "for batch_idx, batch in enumerate(train_dataloader):\n",
    "    print(f\"Processing batch {batch_idx}\")\n",
    "\n",
    "    allocated_before = torch.cuda.memory_allocated() / 1e6\n",
    "    reserved_before = torch.cuda.memory_reserved() / 1e6\n",
    "\n",
    "    # Move batch to GPU and flatten it\n",
    "    batch_data = batch[\"data_tensor\"].float().to(device, non_blocking=True)  # Shape: [B, 132, 175, 48]\n",
    "    batch_data = batch_data.view(batch_data.size(0), -1)  # Flatten to [B, num_features]\n",
    "\n",
    "    # Update running mean\n",
    "    batch_mean = batch_data.mean(dim=0)\n",
    "    num_samples += batch_data.shape[0]\n",
    "    mean_running = (mean_running * (num_samples - batch_data.shape[0]) + batch_data.shape[0] * batch_mean) / num_samples\n",
    "\n",
    "    # Center batch data (remove mean)\n",
    "    batch_data -= mean_running\n",
    "\n",
    "    # Incremental PCA update using Power Iteration\n",
    "    batch_projection = batch_data @ W  # Project onto existing components\n",
    "    U, S, Vh = torch.linalg.svd(batch_projection, full_matrices=False)  # Compute new principal directions\n",
    "    W = (W @ Vh[:n_components].T)  # Update principal components\n",
    "\n",
    "    # Log GPU memory after processing batch\n",
    "    allocated_after = torch.cuda.memory_allocated() / 1e6\n",
    "    reserved_after = torch.cuda.memory_reserved() / 1e6\n",
    "\n",
    "    print(f\"Batch {batch_idx} GPU memory usage:\")\n",
    "    print(f\"  Before processing: allocated={allocated_before:.2f} MB, reserved={reserved_before:.2f} MB\")\n",
    "    print(f\"  After processing: allocated={allocated_after:.2f} MB, reserved={reserved_after:.2f} MB\")\n",
    "\n",
    "    # Free GPU memory\n",
    "    del batch_data, batch_projection, U, S, Vh, batch_mean\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"Incremental PCA training complete. Principal components shape: {W.shape}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepemotion-r8YRC923-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
