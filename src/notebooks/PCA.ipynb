{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_data_loaders\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhydra\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.dataset import get_data_loaders\n",
    "import hydra\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from hydra import initialize, compose\n",
    "import torch\n",
    "import os\n",
    "from utils.dataset import get_data_loaders\n",
    "from models.CNN import CNN\n",
    "from models.resnet import ResNet, BasicBlock\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from utils.dataset import get_data_loaders\n",
    "from models.CNN import CNN\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"CUDA is available. Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA is NOT available. Using CPU.\")\n",
    "\n",
    "hydra.core.global_hydra.GlobalHydra.instance().clear()\n",
    "with initialize(version_base=None, config_path=\"../../src/configs\"):\n",
    "    cfg = compose(config_name=\"base\", overrides=[\"project_root=/home/paperspace/DeepEmotion\"])\n",
    "print(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset contains 152 files.\n",
      "Spatial dimensions: (132, 175, 48)\n",
      "Maximum timepoints per file: 542\n",
      "Subjects: ['sub-01' 'sub-02' 'sub-03' 'sub-04' 'sub-05' 'sub-06' 'sub-07' 'sub-08'\n",
      " 'sub-09' 'sub-11' 'sub-12' 'sub-13' 'sub-14' 'sub-15' 'sub-16' 'sub-17'\n",
      " 'sub-18' 'sub-19' 'sub-20']\n",
      "Sessions: ['01' '02' '03' '04' '05' '06' '07' '08']\n",
      "Emotion categories: ['NONE', 'HAPPINESS', 'FEAR', 'SADNESS', 'LOVE', 'ANGER']\n",
      "Total valid labeled timepoints: 13813\n",
      "Cell 2 complete: Loaded the dataset.\n",
      "Number of train batches: 442\n",
      "Shape of one batch of data: torch.Size([25, 132, 175, 48])\n",
      "batch_size: 25\n"
     ]
    }
   ],
   "source": [
    "train_dataloader, val_dataloader = get_data_loaders(cfg)\n",
    "\n",
    "num_train_batches = len(train_dataloader)\n",
    "sample_batch = next(iter(train_dataloader))\n",
    "batch_data = sample_batch[\"data_tensor\"]\n",
    "\n",
    "print(f\"Cell 2 complete: Loaded the dataset.\")\n",
    "print(f\"Number of train batches: {num_train_batches}\")\n",
    "print(f\"Shape of one batch of data: {batch_data.shape}\")\n",
    "print(f\"batch_size: {cfg.train.batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: '/home/paperspace/DeepEmotion/src/output/PCA/raw'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 53\u001b[0m\n\u001b[1;32m     51\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(results, columns\u001b[38;5;241m=\u001b[39mcolumns)\n\u001b[1;32m     52\u001b[0m csv_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/paperspace/DeepEmotion/src/output/PCA/raw/sub_ALL_ao_annotations.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 53\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPCA projections saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcsv_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deepemotion-r8YRC923-py3.12/lib/python3.12/site-packages/pandas/util/_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    332\u001b[0m     )\n\u001b[0;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deepemotion-r8YRC923-py3.12/lib/python3.12/site-packages/pandas/core/generic.py:3967\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3956\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[1;32m   3958\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[1;32m   3959\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[1;32m   3960\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3964\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[1;32m   3965\u001b[0m )\n\u001b[0;32m-> 3967\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3968\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3969\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3970\u001b[0m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3972\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3973\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3974\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3975\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3976\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3977\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3978\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3979\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3980\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3981\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3982\u001b[0m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3983\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3984\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deepemotion-r8YRC923-py3.12/lib/python3.12/site-packages/pandas/io/formats/format.py:1014\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m    993\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    995\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[1;32m    996\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[1;32m    997\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[1;32m   1013\u001b[0m )\n\u001b[0;32m-> 1014\u001b[0m \u001b[43mcsv_formatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[1;32m   1017\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deepemotion-r8YRC923-py3.12/lib/python3.12/site-packages/pandas/io/formats/csvs.py:251\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;124;03mCreate the writer & save.\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[1;32m    261\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[1;32m    262\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    267\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[1;32m    268\u001b[0m     )\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save()\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deepemotion-r8YRC923-py3.12/lib/python3.12/site-packages/pandas/io/common.py:749\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;66;03m# Only for write methods\u001b[39;00m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m is_path:\n\u001b[0;32m--> 749\u001b[0m     \u001b[43mcheck_parent_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    751\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compression:\n\u001b[1;32m    752\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzstd\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m         \u001b[38;5;66;03m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deepemotion-r8YRC923-py3.12/lib/python3.12/site-packages/pandas/io/common.py:616\u001b[0m, in \u001b[0;36mcheck_parent_directory\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    614\u001b[0m parent \u001b[38;5;241m=\u001b[39m Path(path)\u001b[38;5;241m.\u001b[39mparent\n\u001b[1;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parent\u001b[38;5;241m.\u001b[39mis_dir():\n\u001b[0;32m--> 616\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124mrf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot save file into a non-existent directory: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mOSError\u001b[0m: Cannot save file into a non-existent directory: '/home/paperspace/DeepEmotion/src/output/PCA/raw'"
     ]
    }
   ],
   "source": [
    "# Invert the label dictionary\n",
    "inverse_emotion_idx = {v: k for k, v in cfg.data.emotion_idx.items()}\n",
    "\n",
    "n_components = 2  # if you're only using 2 now\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Get dataset dimensions\n",
    "sample_batch = next(iter(train_dataloader))[\"data_tensor\"]\n",
    "num_features = sample_batch.shape[1] * sample_batch.shape[2] * sample_batch.shape[3]  # e.g., 132 * 175 * 48\n",
    "\n",
    "W = torch.randn(num_features, n_components, device=device)\n",
    "mean_running = torch.zeros(num_features, device=device)\n",
    "num_samples = 0\n",
    "results = []\n",
    "\n",
    "for batch_idx, batch in enumerate(train_dataloader):\n",
    "    batch_data = batch[\"data_tensor\"].float().to(device, non_blocking=True)\n",
    "    batch_data = batch_data.view(batch_data.size(0), -1)\n",
    "\n",
    "    batch_mean = batch_data.mean(dim=0)\n",
    "    num_samples += batch_data.shape[0]\n",
    "    mean_running = (mean_running * (num_samples - batch_data.shape[0]) \n",
    "                    + batch_data.shape[0] * batch_mean) / num_samples\n",
    "\n",
    "    batch_data -= mean_running\n",
    "\n",
    "    # Power iteration / simple incremental update\n",
    "    temp_projection = batch_data @ W\n",
    "    U, S, Vh = torch.linalg.svd(temp_projection, full_matrices=False)\n",
    "    W = W @ Vh[:n_components].T  # W remains [num_features, 2]\n",
    "\n",
    "    batch_projection = batch_data @ W  # shape [B, 2]\n",
    "\n",
    "    # Retrieve labels\n",
    "    labels = batch[\"label_tensor\"]\n",
    "    \n",
    "    # Convert to CPU for final results\n",
    "    batch_projection_cpu = batch_projection.cpu().numpy()\n",
    "    labels_cpu = labels.cpu().numpy()\n",
    "\n",
    "    # Store PC values plus label name\n",
    "    for i in range(batch_projection_cpu.shape[0]):\n",
    "        pc_values = list(batch_projection_cpu[i])  \n",
    "        label_idx = labels_cpu[i]\n",
    "        label_name = inverse_emotion_idx.get(label_idx, \"UNK\")\n",
    "        pc_values.append(label_name)\n",
    "        results.append(pc_values)\n",
    "\n",
    "# Write to CSV\n",
    "columns = [f\"PC{i+1}\" for i in range(n_components)] + [\"EmotionLabel\"]\n",
    "df = pd.DataFrame(results, columns=columns)\n",
    "csv_path = \"/home/paperspace/DeepEmotion/output/PCA/raw/sub_ALL.csv\"\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"PCA projections saved to {csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/paperspace/DeepEmotion/zarr_datasets/pool_emotions\n",
      "Dataset contains 152 files.\n",
      "Spatial dimensions: (132, 175, 48)\n",
      "Maximum timepoints per file: 542\n",
      "Subjects: ['sub-01' 'sub-02' 'sub-03' 'sub-04' 'sub-05' 'sub-06' 'sub-07' 'sub-08'\n",
      " 'sub-09' 'sub-11' 'sub-12' 'sub-13' 'sub-14' 'sub-15' 'sub-16' 'sub-17'\n",
      " 'sub-18' 'sub-19' 'sub-20']\n",
      "Sessions: ['01' '02' '03' '04' '05' '06' '07' '08']\n",
      "Emotion categories: ['NONE', 'HAPPINESS', 'FEAR', 'SADNESS', 'LOVE', 'ANGER']\n",
      "Total valid labeled timepoints: 13813\n",
      "\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting IncrementalPCA Epoch 1: 100%|██████████| 553/553 [06:20<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting IncrementalPCA Epoch 2: 100%|██████████| 553/553 [06:13<00:00,  1.48it/s]\n",
      "Transforming after final epoch: 100%|██████████| 553/553 [06:19<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PCA to /home/paperspace/DeepEmotion/output/PCA/hidden/sub_AL_ao_annotations.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from omegaconf import OmegaConf\n",
    "from utils.dataset import get_data_loaders\n",
    "from models.CNN import CNN\n",
    "\n",
    "# Config variables\n",
    "PROJECT_ROOT = os.path.abspath(\"../../\")\n",
    "\n",
    "cfg = OmegaConf.create({\n",
    "    \"project_root\": PROJECT_ROOT,\n",
    "    \"verbose\": True,\n",
    "    \"wandb\": True,\n",
    "    \"sys_log\": True,\n",
    "    \"model\": \"CNN\",\n",
    "    \"CNN\": {\n",
    "        \"c1\": 16, \"c2\": 32, \"c3\": 64, \"k1\": 3, \"k2\": 3, \"k3\": 3,\n",
    "        \"pk\": 2, \"ps\": 2, \"kernel_size\": 3, \"stride\": 1, \"padding\": 1\n",
    "    },\n",
    "    \"train\": {\n",
    "        \"epochs\": 50, \"batch_size\": 20, \"shuffle\": True, \"train_ratio\": 0.8,\n",
    "        \"print_label_frequencies\": True\n",
    "    },\n",
    "    \"data\": {\n",
    "        \"data_path\": f\"{PROJECT_ROOT}/data/raw/derivatives/non-linear_anatomical_alignment\",\n",
    "        \"zarr_dir_path\": f\"{PROJECT_ROOT}/zarr_datasets\",\n",
    "        \"zarr_path\": f\"{PROJECT_ROOT}/zarr_datasets/pool_emotions\",\n",
    "        \"label_path\": f\"{PROJECT_ROOT}/data/updated_annotations/pooled_annotations_structured.tsv\",\n",
    "        \"sessions\": [\"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\"],\n",
    "        \"file_pattern_template\": \"*_ses-forrestgump_task-forrestgump_rec-dico7Tad2grpbold7TadNL_run-{}_bold.nii.gz\",\n",
    "        \"subjects\": [\"sub-7\"],\n",
    "        \"session_offsets\": [0, 902, 1784, 2660, 3636, 4560, 5438, 6522],\n",
    "        \"emotion_idx\": {\"NONE\": 0, \"HAPPINESS\": 1, \"FEAR\": 2, \"SADNESS\": 3, \"LOVE\": 4, \"ANGER\": 5},\n",
    "        \"normalization\": False,\n",
    "        \"weight_decay\": 0,\n",
    "        \"learning_rate\": 0.0001,\n",
    "        \"seed\": 42,\n",
    "        \"save_model\": True,\n",
    "        \"load_model\": False,\n",
    "        \"save_model_path\": \"output/models\",\n",
    "        \"load_model_path\": f\"{PROJECT_ROOT}/output/models/model-sub02-20.pth\",\n",
    "        \"output_csv_path\": f\"{PROJECT_ROOT}/output/PCA/hidden/sub_ALL.csv\"\n",
    "    }\n",
    "})\n",
    "\n",
    "print(cfg.data.zarr_path)\n",
    "\n",
    "# Load dataloaders\n",
    "train_dataloader, val_dataloader = get_data_loaders(cfg)\n",
    "\n",
    "# Load model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CNN(cfg=cfg, output_dim=len(cfg.data.emotion_idx))\n",
    "model.load_state_dict(torch.load(cfg.data.load_model_path, map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Get hidden state dimension\n",
    "sample_batch = next(iter(train_dataloader))[\"data_tensor\"].to(device).float()\n",
    "with torch.no_grad():\n",
    "    _, hidden_sample = model(sample_batch[:1], return_hidden=True)\n",
    "hidden_dim = hidden_sample.shape[1]\n",
    "\n",
    "# Invert emotion label map\n",
    "inverse_emotion_idx = {v: k for k, v in cfg.data.emotion_idx.items()}\n",
    "\n",
    "# PCA loop\n",
    "n_components = 2\n",
    "num_epochs = 2\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    print(f\"\\nEpoch {epoch}/{num_epochs}\")\n",
    "    \n",
    "    ipca = IncrementalPCA(n_components=n_components)\n",
    "\n",
    "    # Fit PCA incrementally\n",
    "    for batch in tqdm(train_dataloader, desc=f\"Fitting IncrementalPCA Epoch {epoch}\"):\n",
    "        data = batch[\"data_tensor\"].float().to(device)\n",
    "        with torch.no_grad():\n",
    "            _, hidden = model(data, return_hidden=True)\n",
    "        ipca.partial_fit(hidden.cpu().numpy())\n",
    "\n",
    "# After final epoch, transform and save\n",
    "all_hidden = []\n",
    "all_labels = []\n",
    "\n",
    "for batch in tqdm(train_dataloader, desc=\"Transforming after final epoch\"):\n",
    "    data = batch[\"data_tensor\"].float().to(device)\n",
    "    labels = batch[\"label_tensor\"]\n",
    "    with torch.no_grad():\n",
    "        _, hidden = model(data, return_hidden=True)\n",
    "    all_hidden.append(hidden.cpu().numpy())\n",
    "    all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "all_hidden = np.concatenate(all_hidden, axis=0)\n",
    "all_labels = np.concatenate(all_labels, axis=0)\n",
    "pca_proj = ipca.transform(all_hidden)\n",
    "\n",
    "results = []\n",
    "for row, label_idx in zip(pca_proj, all_labels):\n",
    "    row = list(row)\n",
    "    row.append(inverse_emotion_idx.get(label_idx, \"UNK\"))\n",
    "    results.append(row)\n",
    "\n",
    "df = pd.DataFrame(results, columns=[\"PC1\", \"PC2\", \"EmotionLabel\"])\n",
    "save_dir = os.path.join(PROJECT_ROOT, \"output/PCA/hidden\")\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "csv_path = os.path.join(save_dir, \"sub_AL_ao_annotations.csv\")\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"Saved PCA to {csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepemotion-r8YRC923-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
