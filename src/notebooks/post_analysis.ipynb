{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcollections\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Counter\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_data_loaders\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mCNN\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CNN\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mresnet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ResNet, BasicBlock\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Assuming notebook is in project root\n",
    "PROJECT_ROOT = Path(\"/scratch/lungerjo/DeepEmotion\")\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from omegaconf import OmegaConf\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from srcutils.dataset import get_data_loaders\n",
    "from models.CNN import CNN\n",
    "from models.resnet import ResNet, BasicBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Dataset contains 8 files.\n",
      "Spatial dimensions: (132, 175, 48)\n",
      "Maximum timepoints per file: 542\n",
      "Subjects: ['sub-20']\n",
      "Sessions: ['01' '02' '03' '04' '05' '06' '07' '08']\n",
      "Emotion categories: ['NONE', 'HAPPINESS', 'FEAR', 'SADNESS', 'LOVE', 'ANGER']\n",
      "Total valid labeled timepoints: 799\n",
      "Loaded Validation Observations: 160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_79149/3863842772.py:85: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path_torch, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from /home/paperspace/DeepEmotion/src/output/models/models/sub_20.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference on Validation Set: 100%|██████████| 8/8 [00:04<00:00,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation predictions saved to: /home/paperspace/DeepEmotion/src/output/inference/sub_20.csv\n",
      "\n",
      "Validation Accuracy per Emotion:\n",
      "NONE: No samples in validation set\n",
      "HAPPINESS: 67.35% (33/49)\n",
      "FEAR: 93.33% (28/30)\n",
      "SADNESS: 77.27% (34/44)\n",
      "LOVE: 68.42% (13/19)\n",
      "ANGER: 61.11% (11/18)\n",
      "\n",
      "Overall Validation Accuracy: 74.38% (119/160)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "# Config variables\n",
    "PROJECT_ROOT = os.path.abspath(\"../\")\n",
    "\n",
    "cfg = OmegaConf.create({\n",
    "    \"project_root\": PROJECT_ROOT,\n",
    "    \"verbose\": True,\n",
    "    \"wandb\": True,\n",
    "    \"sys_log\": True,\n",
    "    \"model\": \"CNN\",\n",
    "    \"CNN\": {\n",
    "        \"c1\": 16, \"c2\": 32, \"c3\": 64, \"k1\": 3, \"k2\": 3, \"k3\": 3,\n",
    "        \"pk\": 2, \"ps\": 2, \"kernel_size\": 3, \"stride\": 1, \"padding\": 1\n",
    "    },\n",
    "    \"train\": {\n",
    "        \"epochs\": 50, \"batch_size\": 20, \"shuffle\": True, \"train_ratio\": 0.8,\n",
    "        \"print_label_frequencies\": True\n",
    "    },\n",
    "    \"data\": {\n",
    "        \"data_path\": f\"{PROJECT_ROOT}/data/raw/derivatives/non-linear_anatomical_alignment\",\n",
    "        \"zarr_dir_path\": f\"{PROJECT_ROOT}/zarr_datasets\",\n",
    "        \"zarr_path\": f\"{PROJECT_ROOT}/zarr_datasets/pool_emotions\",\n",
    "        \"label_path\": f\"{PROJECT_ROOT}/data/updated_annotations/pooled_annotations_structured.tsv\",\n",
    "        \"sessions\": [\"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\"],\n",
    "        \"file_pattern_template\": \"*_ses-forrestgump_task-forrestgump_rec-dico7Tad2grpbold7TadNL_run-{}_bold.nii.gz\",\n",
    "        \"subjects\": [\"sub-01\"],\n",
    "        \"session_offsets\": [0, 902, 1784, 2660, 3636, 4560, 5438, 6522],\n",
    "        \"emotion_idx\": {\"NONE\": 0, \"HAPPINESS\": 1, \"FEAR\": 2, \"SADNESS\": 3, \"LOVE\": 4, \"ANGER\": 5},\n",
    "        \"weight_decay\": 0,\n",
    "        \"learning_rate\": 0.0001,\n",
    "        \"seed\": 42,\n",
    "        \"save_model\": True,\n",
    "        \"load_model\": False,\n",
    "        \"save_model_path\": \"output/models\",\n",
    "        \"load_model_path\": f\"{PROJECT_ROOT}/src/output/models/models/sub_20.pth\",\n",
    "        \"output_csv_path\": f\"{PROJECT_ROOT}/src/output/inference/sub_20.csv\"\n",
    "    }\n",
    "})\n",
    "\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    predictions_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Running Inference on Validation Set\"):\n",
    "            data, labels = batch[\"data_tensor\"], batch[\"label_tensor\"]\n",
    "            data = data.float().to(device)\n",
    "            labels = labels.long().to(device)\n",
    "            if data.dim() == 4:\n",
    "                data = data.unsqueeze(1)  # Ensure correct input shape\n",
    "            output = model(data)\n",
    "            _, predictions = torch.max(output, dim=1)\n",
    "\n",
    "            # Collect results in list\n",
    "            for true_label, pred in zip(labels.cpu().numpy(), predictions.cpu().numpy()):\n",
    "                predictions_list.append([true_label, pred])\n",
    "    \n",
    "    return predictions_list\n",
    "\n",
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    train_dataloader, val_dataloader = get_data_loaders(cfg)\n",
    "    print(f\"Loaded Validation Observations: {len(val_dataloader.dataset)}\")\n",
    "    \n",
    "    output_dim = len(cfg.data.emotion_idx)\n",
    "    model_path_torch = cfg.data.load_model_path\n",
    "    \n",
    "    if cfg.model == \"CNN\":\n",
    "        model = CNN(cfg=cfg, output_dim=output_dim)\n",
    "    elif cfg.model == \"ResNet\":\n",
    "        model = ResNet(BasicBlock, [1, 1, 1, 1], in_channels=1, num_classes=output_dim)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model specified\")\n",
    "    \n",
    "    if model_path_torch:\n",
    "        model.load_state_dict(torch.load(model_path_torch, map_location=device))\n",
    "        print(f\"Loaded model from {model_path_torch}\")\n",
    "    \n",
    "    model.to(device)\n",
    "\n",
    "    # Run inference\n",
    "    val_predictions = evaluate_model(model, val_dataloader, device)\n",
    "    \n",
    "    # Convert label indices to emotion names\n",
    "    emotion_idx = cfg.data.emotion_idx\n",
    "    inverse_emotion_idx = {v: k for k, v in emotion_idx.items()}  # Reverse mapping\n",
    "\n",
    "    val_predictions_named = [\n",
    "        [inverse_emotion_idx[true_label], inverse_emotion_idx[predicted]]\n",
    "        for true_label, predicted in val_predictions\n",
    "    ]\n",
    "\n",
    "    # Save to CSV\n",
    "    output_csv_path = cfg.data.output_csv_path\n",
    "    df = pd.DataFrame(val_predictions_named, columns=[\"true_label\", \"predicted\"])\n",
    "    os.makedirs(os.path.dirname(output_csv_path), exist_ok=True)\n",
    "    df.to_csv(output_csv_path, index=False)\n",
    "    \n",
    "    print(f\"Validation predictions saved to: {output_csv_path}\")\n",
    "\n",
    "    # Compute per-class accuracy\n",
    "    total_counts = {k: 0 for k in emotion_idx.keys()}  # Total per-class samples\n",
    "    correct_counts = {k: 0 for k in emotion_idx.keys()}  # Correct per-class predictions\n",
    "\n",
    "    for true_label, predicted in val_predictions_named:\n",
    "        total_counts[true_label] += 1\n",
    "        if true_label == predicted:\n",
    "            correct_counts[true_label] += 1\n",
    "\n",
    "    # Print per-class accuracy\n",
    "    print(\"\\nValidation Accuracy per Emotion:\")\n",
    "    for emotion, total in total_counts.items():\n",
    "        if total > 0:\n",
    "            accuracy = (correct_counts[emotion] / total) * 100\n",
    "            print(f\"{emotion}: {accuracy:.2f}% ({correct_counts[emotion]}/{total})\")\n",
    "        else:\n",
    "            print(f\"{emotion}: No samples in validation set\")\n",
    "\n",
    "    # Print overall validation accuracy\n",
    "    total_correct = sum(correct_counts.values())\n",
    "    total_samples = sum(total_counts.values())\n",
    "    overall_accuracy = (total_correct / total_samples) * 100 if total_samples > 0 else 0\n",
    "    print(f\"\\nOverall Validation Accuracy: {overall_accuracy:.2f}% ({total_correct}/{total_samples})\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "# Config variables\n",
    "PROJECT_ROOT = os.path.abspath(\"../\")\n",
    "\n",
    "cfg = OmegaConf.create({\n",
    "    \"project_root\": PROJECT_ROOT,\n",
    "    \"verbose\": True,\n",
    "    \"wandb\": True,\n",
    "    \"sys_log\": True,\n",
    "    \"model\": \"CNN\",\n",
    "    \"CNN\": {\n",
    "        \"c1\": 16, \"c2\": 32, \"c3\": 64, \"k1\": 3, \"k2\": 3, \"k3\": 3,\n",
    "        \"pk\": 2, \"ps\": 2, \"kernel_size\": 3, \"stride\": 1, \"padding\": 1\n",
    "    },\n",
    "    \"train\": {\n",
    "        \"epochs\": 50, \"batch_size\": 20, \"shuffle\": True, \"train_ratio\": 0.8,\n",
    "        \"print_label_frequencies\": True\n",
    "    },\n",
    "    \"data\": {\n",
    "        \"data_path\": f\"{PROJECT_ROOT}/data/raw/derivatives/non-linear_anatomical_alignment\",\n",
    "        \"zarr_dir_path\": f\"{PROJECT_ROOT}/zarr_datasets\",\n",
    "        \"zarr_path\": f\"{PROJECT_ROOT}/zarr_datasets/pool_emotions\",\n",
    "        \"label_path\": f\"{PROJECT_ROOT}/data/updated_annotations/pooled_annotations_structured.tsv\",\n",
    "        \"sessions\": [\"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\"],\n",
    "        \"file_pattern_template\": \"*_ses-forrestgump_task-forrestgump_rec-dico7Tad2grpbold7TadNL_run-{}_bold.nii.gz\",\n",
    "        \"subjects\": [\"sub-20\"],\n",
    "        \"session_offsets\": [0, 902, 1784, 2660, 3636, 4560, 5438, 6522],\n",
    "        \"emotion_idx\": {\"NONE\": 0, \"HAPPINESS\": 1, \"FEAR\": 2, \"SADNESS\": 3, \"LOVE\": 4, \"ANGER\": 5},\n",
    "        \"normalization\": False,\n",
    "        \"weight_decay\": 0,\n",
    "        \"learning_rate\": 0.0001,\n",
    "        \"seed\": 42,\n",
    "        \"save_model\": True,\n",
    "        \"load_model\": False,\n",
    "        \"save_model_path\": \"output/models\",\n",
    "        \"load_model_path\": f\"{PROJECT_ROOT}/src/output/models/models/sub_20.pth\",\n",
    "        \"output_csv_path\": f\"{PROJECT_ROOT}/src/output/inference/sub_20.csv\"\n",
    "    }\n",
    "})\n",
    "\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    predictions_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Running Inference on Validation Set\"):\n",
    "            data, labels = batch[\"data_tensor\"], batch[\"label_tensor\"]\n",
    "            data = data.float().to(device)\n",
    "            labels = labels.long().to(device)\n",
    "            if data.dim() == 4:\n",
    "                data = data.unsqueeze(1)  # Ensure correct input shape\n",
    "            output = model(data)\n",
    "            _, predictions = torch.max(output, dim=1)\n",
    "\n",
    "            # Collect results in list\n",
    "            for true_label, pred in zip(labels.cpu().numpy(), predictions.cpu().numpy()):\n",
    "                predictions_list.append([true_label, pred])\n",
    "    \n",
    "    return predictions_list\n",
    "\n",
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    train_dataloader, val_dataloader = get_data_loaders(cfg)\n",
    "    print(f\"Loaded Validation Observations: {len(val_dataloader.dataset)}\")\n",
    "    \n",
    "    output_dim = len(cfg.data.emotion_idx)\n",
    "    model_path_torch = cfg.data.load_model_path\n",
    "    \n",
    "    if cfg.model == \"CNN\":\n",
    "        model = CNN(cfg=cfg, output_dim=output_dim)\n",
    "    elif cfg.model == \"ResNet\":\n",
    "        model = ResNet(BasicBlock, [1, 1, 1, 1], in_channels=1, num_classes=output_dim)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model specified\")\n",
    "    \n",
    "    if model_path_torch:\n",
    "        model.load_state_dict(torch.load(model_path_torch, map_location=device))\n",
    "        print(f\"Loaded model from {model_path_torch}\")\n",
    "    \n",
    "    model.to(device)\n",
    "\n",
    "    # Run inference\n",
    "    val_predictions = evaluate_model(model, val_dataloader, device)\n",
    "    \n",
    "    # Convert label indices to emotion names\n",
    "    emotion_idx = cfg.data.emotion_idx\n",
    "    inverse_emotion_idx = {v: k for k, v in emotion_idx.items()}  # Reverse mapping\n",
    "\n",
    "    val_predictions_named = [\n",
    "        [inverse_emotion_idx[true_label], inverse_emotion_idx[predicted]]\n",
    "        for true_label, predicted in val_predictions\n",
    "    ]\n",
    "\n",
    "    # Save to CSV\n",
    "    output_csv_path = cfg.data.output_csv_path\n",
    "    df = pd.DataFrame(val_predictions_named, columns=[\"true_label\", \"predicted\"])\n",
    "    os.makedirs(os.path.dirname(output_csv_path), exist_ok=True)\n",
    "    df.to_csv(output_csv_path, index=False)\n",
    "    \n",
    "    print(f\"Validation predictions saved to: {output_csv_path}\")\n",
    "\n",
    "    # Compute per-class accuracy\n",
    "    total_counts = {k: 0 for k in emotion_idx.keys()}  # Total per-class samples\n",
    "    correct_counts = {k: 0 for k in emotion_idx.keys()}  # Correct per-class predictions\n",
    "\n",
    "    for true_label, predicted in val_predictions_named:\n",
    "        total_counts[true_label] += 1\n",
    "        if true_label == predicted:\n",
    "            correct_counts[true_label] += 1\n",
    "\n",
    "    # Print per-class accuracy\n",
    "    print(\"\\nValidation Accuracy per Emotion:\")\n",
    "    for emotion, total in total_counts.items():\n",
    "        if total > 0:\n",
    "            accuracy = (correct_counts[emotion] / total) * 100\n",
    "            print(f\"{emotion}: {accuracy:.2f}% ({correct_counts[emotion]}/{total})\")\n",
    "        else:\n",
    "            print(f\"{emotion}: No samples in validation set\")\n",
    "\n",
    "    # Print overall validation accuracy\n",
    "    total_correct = sum(correct_counts.values())\n",
    "    total_samples = sum(total_counts.values())\n",
    "    overall_accuracy = (total_correct / total_samples) * 100 if total_samples > 0 else 0\n",
    "    print(f\"\\nOverall Validation Accuracy: {overall_accuracy:.2f}% ({total_correct}/{total_samples})\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepemotion-r8YRC923-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
