{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/paperspace/DeepEmotion/src/output/models/sub_ALL.pth\n",
      "Loaded model from /home/paperspace/DeepEmotion/src/output/models/sub_ALL.pth\n",
      "Time 206.0s, volume_idx=0, row_idx=103 Subject=sub-02, Session=01 | True=HAPPINESS, Predicted=HAPPINESS\n",
      "Time 206.0s, volume_idx=1, row_idx=103 Subject=sub-03, Session=01 | True=HAPPINESS, Predicted=HAPPINESS\n",
      "Time 206.0s, volume_idx=2, row_idx=103 Subject=sub-04, Session=01 | True=HAPPINESS, Predicted=HAPPINESS\n",
      "Time 206.0s, volume_idx=3, row_idx=103 Subject=sub-05, Session=01 | True=HAPPINESS, Predicted=HAPPINESS\n",
      "Time 206.0s, volume_idx=4, row_idx=103 Subject=sub-06, Session=01 | True=HAPPINESS, Predicted=HAPPINESS\n",
      "Time 206.0s, volume_idx=5, row_idx=103 Subject=sub-07, Session=01 | True=HAPPINESS, Predicted=LOVE\n",
      "Time 206.0s, volume_idx=6, row_idx=103 Subject=sub-08, Session=01 | True=HAPPINESS, Predicted=HAPPINESS\n",
      "Time 206.0s, volume_idx=7, row_idx=103 Subject=sub-09, Session=01 | True=HAPPINESS, Predicted=HAPPINESS\n",
      "Time 206.0s, volume_idx=8, row_idx=103 Subject=sub-11, Session=01 | True=HAPPINESS, Predicted=HAPPINESS\n",
      "Time 206.0s, volume_idx=9, row_idx=103 Subject=sub-12, Session=01 | True=HAPPINESS, Predicted=HAPPINESS\n",
      "Time 206.0s, volume_idx=10, row_idx=103 Subject=sub-13, Session=01 | True=HAPPINESS, Predicted=HAPPINESS\n",
      "Time 206.0s, volume_idx=11, row_idx=103 Subject=sub-14, Session=01 | True=HAPPINESS, Predicted=HAPPINESS\n",
      "Time 206.0s, volume_idx=12, row_idx=103 Subject=sub-15, Session=01 | True=HAPPINESS, Predicted=HAPPINESS\n",
      "Time 206.0s, volume_idx=13, row_idx=103 Subject=sub-16, Session=01 | True=HAPPINESS, Predicted=HAPPINESS\n",
      "Time 206.0s, volume_idx=14, row_idx=103 Subject=sub-17, Session=01 | True=HAPPINESS, Predicted=HAPPINESS\n",
      "Time 206.0s, volume_idx=15, row_idx=103 Subject=sub-18, Session=01 | True=HAPPINESS, Predicted=HAPPINESS\n",
      "Time 206.0s, volume_idx=16, row_idx=103 Subject=sub-19, Session=01 | True=HAPPINESS, Predicted=HAPPINESS\n",
      "Time 206.0s, volume_idx=17, row_idx=103 Subject=sub-20, Session=01 | True=HAPPINESS, Predicted=HAPPINESS\n"
     ]
    }
   ],
   "source": [
    "# In[ ]:\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "from utils.dataset import ZarrDataset\n",
    "from models.CNN import CNN\n",
    "from models.resnet import ResNet, BasicBlock\n",
    "\n",
    "def build_offset_map(zarr_dataset):\n",
    "    \"\"\"\n",
    "    Creates a dictionary of offset -> list of (volume_idx, row_idx) for quick lookup.\n",
    "    \"\"\"\n",
    "    offset_to_indices = {}\n",
    "    if zarr_dataset.aligned_labels is None:\n",
    "        print(\"No aligned_labels found in ZarrDataset. Make sure 'aligned_labels' attribute is present.\")\n",
    "        return offset_to_indices\n",
    "\n",
    "    # For each row in aligned_labels, gather (volume_idx, row_index).\n",
    "    for i, row in zarr_dataset.aligned_labels.iterrows():\n",
    "        offset = row['time_offset']\n",
    "        vol_idx = row['file_index']\n",
    "        t_idx = row['row_index']\n",
    "        offset_to_indices.setdefault(offset, []).append((vol_idx, t_idx))\n",
    "\n",
    "    return offset_to_indices\n",
    "\n",
    "def forward_model_on_offset(model, zarr_dataset, offset, offset_map, device):\n",
    "    \"\"\"\n",
    "    Find all samples in zarr_dataset for a given time offset, run them through the model,\n",
    "    and return a list of predictions and any metadata you want to track.\n",
    "    \"\"\"\n",
    "    # Find all (volume_idx, row_idx) pairs for this offset\n",
    "    if offset not in offset_map:\n",
    "        print(f\"No samples found for offset {offset}\")\n",
    "        return []\n",
    "\n",
    "    results = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for (vol_idx, row_idx) in offset_map[offset]:\n",
    "            # We must find the dataset index (the \"flattened\" index) that corresponds to (vol_idx, row_idx).\n",
    "            # Because ZarrDataset.__getitem__ uses self.valid_indices, we look up the pair in valid_indices:\n",
    "            try:\n",
    "                dataset_index = zarr_dataset.valid_indices.index((vol_idx, row_idx))\n",
    "            except ValueError:\n",
    "                # This means (vol_idx, row_idx) wasn't in valid_indices (it might be an unlabeled sample)\n",
    "                continue\n",
    "            \n",
    "            item = zarr_dataset[dataset_index]\n",
    "            data_tensor = item[\"data_tensor\"].unsqueeze(0).float().to(device)\n",
    "            if data_tensor.dim() == 4:\n",
    "                # Model expects (batch, channels, x, y, z)\n",
    "                data_tensor = data_tensor.unsqueeze(1)\n",
    "\n",
    "            output = model(data_tensor)\n",
    "            _, predicted_idx = torch.max(output, dim=1)\n",
    "            predicted_idx = predicted_idx.item()\n",
    "\n",
    "            results.append({\n",
    "                \"volume_idx\": vol_idx,\n",
    "                \"row_idx\": row_idx,\n",
    "                \"time_offset\": item[\"time_offset\"],\n",
    "                \"true_label_idx\": item[\"label_tensor\"].item(),\n",
    "                \"predicted_idx\": predicted_idx,\n",
    "                \"subject\": item[\"subject\"],\n",
    "                \"session\": item[\"session\"]\n",
    "            })\n",
    "    return results\n",
    "\n",
    "# -------------------- Example usage --------------------\n",
    "\n",
    "# 1) Build an in-memory config (or load your real Hydra config)\n",
    "PROJECT_ROOT = os.path.abspath(\"..\")\n",
    "cfg = OmegaConf.create({\n",
    "    \"project_root\": PROJECT_ROOT,\n",
    "    \"verbose\": True,\n",
    "    \"wandb\": True,\n",
    "    \"sys_log\": True,\n",
    "    \"model\": \"CNN\",\n",
    "    \"CNN\": {\n",
    "        \"c1\": 16, \"c2\": 32, \"c3\": 64, \"k1\": 3, \"k2\": 3, \"k3\": 3,\n",
    "        \"pk\": 2, \"ps\": 2, \"kernel_size\": 3, \"stride\": 1, \"padding\": 1\n",
    "    },\n",
    "    \"train\": {\n",
    "        \"epochs\": 50, \"batch_size\": 20, \"shuffle\": True, \"train_ratio\": 0.8,\n",
    "        \"print_label_frequencies\": True\n",
    "    },\n",
    "    \"data\": {\n",
    "        \"data_path\": f\"{PROJECT_ROOT}/data/raw/derivatives/non-linear_anatomical_alignment\",\n",
    "        \"zarr_dir_path\": f\"{PROJECT_ROOT}/zarr_datasets\",\n",
    "        \"zarr_path\": f\"{PROJECT_ROOT}/zarr_datasets/pool_emotions\",\n",
    "        \"label_path\": f\"{PROJECT_ROOT}/data/updated_annotations/pooled_annotations_structured.tsv\",\n",
    "        \"sessions\": [\"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\"],\n",
    "        \"file_pattern_template\": \"*_ses-forrestgump_task-forrestgump_rec-dico7Tad2grpbold7TadNL_run-{}_bold.nii.gz\",\n",
    "        \"subjects\": [\"sub-20\"],\n",
    "        \"session_offsets\": [0, 902, 1784, 2660, 3636, 4560, 5438, 6522],\n",
    "        \"emotion_idx\": {\"NONE\": 0, \"HAPPINESS\": 1, \"FEAR\": 2, \"SADNESS\": 3, \"LOVE\": 4, \"ANGER\": 5},\n",
    "        \"normalization\": False,\n",
    "        \"weight_decay\": 0,\n",
    "        \"learning_rate\": 0.0001,\n",
    "        \"seed\": 42,\n",
    "        \"save_model\": True,\n",
    "        \"load_model\": True,\n",
    "        \"save_model_path\": \"output/models\",\n",
    "        \"load_model_path\": f\"{PROJECT_ROOT}/src/output/models/sub_ALL.pth\",\n",
    "        \"output_csv_path\": f\"{PROJECT_ROOT}/src/output/inference/sub_ALL.csv\"\n",
    "    }\n",
    "})\n",
    "\n",
    "print(cfg.data.load_model_path)\n",
    "\n",
    "# 2) Load dataset\n",
    "zarr_dataset = ZarrDataset(cfg.data.zarr_path)\n",
    "\n",
    "# 3) Build the offset map for quick lookups\n",
    "offset_map = build_offset_map(zarr_dataset)\n",
    "\n",
    "# 4) Instantiate the model and load weights\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "emotion_idx = cfg.data.emotion_idx\n",
    "output_dim = len(emotion_idx)\n",
    "\n",
    "if cfg.model == \"CNN\":\n",
    "    from models.CNN import CNN\n",
    "    model = CNN(cfg=cfg, output_dim=output_dim)\n",
    "elif cfg.model == \"ResNet\":\n",
    "    from models.resnet import ResNet, BasicBlock\n",
    "    model = ResNet(BasicBlock, [1, 1, 1, 1], in_channels=1, num_classes=output_dim)\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported model: {cfg.model}\")\n",
    "\n",
    "model_path = cfg.data.load_model_path\n",
    "if os.path.exists(model_path):\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    print(f\"Loaded model from {model_path}\")\n",
    "model.to(device)\n",
    "\n",
    "# 5) Run inference for a chosen offset\n",
    "sample_offset = 206.0  # seconds into the movie, for instance\n",
    "results = forward_model_on_offset(model, zarr_dataset, sample_offset, offset_map, device)\n",
    "\n",
    "# 6) Map prediction indices back to emotion names\n",
    "inv_idx = {v: k for k, v in emotion_idx.items()}\n",
    "\n",
    "for r in results:\n",
    "    pred_name = inv_idx[r[\"predicted_idx\"]]\n",
    "    true_name = inv_idx[r[\"true_label_idx\"]]\n",
    "    print(f\"Time {r['time_offset']:.1f}s, volume_idx={r['volume_idx']}, row_idx={r['row_idx']} \"\n",
    "          f\"Subject={r['subject']}, Session={r['session']} | \"\n",
    "          f\"True={true_name}, Predicted={pred_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples (including NONE): 14382\n",
      "Loaded model weights from /home/paperspace/DeepEmotion/src/output/models/sub_ALL.pth\n",
      "Pass 1: partial_fit on hidden states for non-NONE samples\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "n_components=2 must be less or equal to the batch number of samples 1 for the first partial_fit call.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 86\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;66;03m# Convert to CPU numpy for IncrementalPCA\u001b[39;00m\n\u001b[1;32m     85\u001b[0m     hidden_np \u001b[38;5;241m=\u001b[39m hidden\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m---> 86\u001b[0m     \u001b[43mipca\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_np\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# 6) Second pass: transform, collect (PC1, PC2) + [Emotion, time_offset, subject]\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPass 2: transform hidden states and store results\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deepemotion-r8YRC923-py3.12/lib/python3.12/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/deepemotion-r8YRC923-py3.12/lib/python3.12/site-packages/sklearn/decomposition/_incremental_pca.py:310\u001b[0m, in \u001b[0;36mIncrementalPCA.partial_fit\u001b[0;34m(self, X, y, check_input)\u001b[0m\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_components=\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m invalid for n_features=\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m, need \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmore rows than columns for IncrementalPCA \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocessing\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components, n_features)\n\u001b[1;32m    308\u001b[0m     )\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components \u001b[38;5;241m>\u001b[39m n_samples \u001b[38;5;129;01mand\u001b[39;00m first_pass:\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    311\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_components=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be less or equal to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    312\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe batch number of samples \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for the first \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    313\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpartial_fit call.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    314\u001b[0m     )\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components\n",
      "\u001b[0;31mValueError\u001b[0m: n_components=2 must be less or equal to the batch number of samples 1 for the first partial_fit call."
     ]
    }
   ],
   "source": [
    "# In[ ]:\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "from utils.dataset import ZarrDataset\n",
    "from models.CNN import CNN\n",
    "from models.resnet import ResNet, BasicBlock\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "###############################################################################\n",
    "# Example: Perform incremental PCA on the full dataset by directly iterating \n",
    "# over ZarrDataset. Produces a CSV with [PC1, PC2, EmotionLabel, time_offset, subject],\n",
    "# sorted first by subject, then by time offset.\n",
    "###############################################################################\n",
    "\n",
    "# 1) Define or load a Hydra config\n",
    "PROJECT_ROOT = os.path.abspath(\"..\")\n",
    "cfg = OmegaConf.create({\n",
    "    \"project_root\": PROJECT_ROOT,\n",
    "    \"model\": \"CNN\",\n",
    "    \"CNN\": {\n",
    "        \"c1\": 16, \"c2\": 32, \"c3\": 64, \"k1\": 3, \"k2\": 3, \"k3\": 3,\n",
    "        \"pk\": 2, \"ps\": 2, \"kernel_size\": 3, \"stride\": 1, \"padding\": 1\n",
    "    },\n",
    "    \"data\": {\n",
    "        \"zarr_path\": f\"{PROJECT_ROOT}/zarr_datasets/pool_emotions\",  \n",
    "        \"emotion_idx\": {\"NONE\": 0, \"HAPPINESS\": 1, \"FEAR\": 2, \"SADNESS\": 3, \"LOVE\": 4, \"ANGER\": 5},\n",
    "        \"load_model_path\": f\"{PROJECT_ROOT}/src/output/models/sub_ALL.pth\"\n",
    "    },\n",
    "    \"pca\": {\n",
    "        \"n_components\": 2\n",
    "    },\n",
    "    \"output\": {\n",
    "        \"csv_dir\": f\"{PROJECT_ROOT}/src/output/PCA/full_dataset\",\n",
    "        \"csv_name\": \"pca_full_dataset.csv\"\n",
    "    }\n",
    "})\n",
    "\n",
    "# 2) Load your ZarrDataset\n",
    "zarr_dataset = ZarrDataset(cfg.data.zarr_path)\n",
    "print(f\"Total samples (including NONE): {len(zarr_dataset)}\")\n",
    "\n",
    "# 3) Build an inverse emotion mapping for easier label naming\n",
    "inverse_emotion_idx = {v: k for k, v in cfg.data.emotion_idx.items()}\n",
    "\n",
    "# 4) Instantiate the model and load weights\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if cfg.model == \"CNN\":\n",
    "    model = CNN(cfg=cfg, output_dim=len(cfg.data.emotion_idx))\n",
    "elif cfg.model == \"ResNet\":\n",
    "    model = ResNet(BasicBlock, [1,1,1,1], in_channels=1, num_classes=len(cfg.data.emotion_idx))\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported model: {cfg.model}\")\n",
    "\n",
    "if os.path.exists(cfg.data.load_model_path):\n",
    "    model.load_state_dict(torch.load(cfg.data.load_model_path, map_location=device))\n",
    "    print(f\"Loaded model weights from {cfg.data.load_model_path}\")\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# 5) Incremental PCA: partial_fit in one pass\n",
    "print(\"Pass 1: partial_fit on hidden states for non-NONE samples\")\n",
    "ipca = IncrementalPCA(n_components=cfg.pca.n_components)\n",
    "\n",
    "for i in range(len(zarr_dataset)):\n",
    "    item = zarr_dataset[i]\n",
    "    label_idx = item[\"label_tensor\"].item()\n",
    "\n",
    "    # Skip samples labeled as NONE\n",
    "    if label_idx == cfg.data.emotion_idx[\"NONE\"]:\n",
    "        continue\n",
    "\n",
    "    data_tensor = item[\"data_tensor\"].unsqueeze(0).float().to(device)\n",
    "    with torch.no_grad():\n",
    "        # model(...) should return (output, hidden) if you have return_hidden=True\n",
    "        # adapt to your actual interface:\n",
    "        _, hidden = model(data_tensor, return_hidden=True)\n",
    "\n",
    "    # Convert to CPU numpy for IncrementalPCA\n",
    "    hidden_np = hidden.cpu().numpy()\n",
    "    ipca.partial_fit(hidden_np)\n",
    "\n",
    "# 6) Second pass: transform, collect (PC1, PC2) + [Emotion, time_offset, subject]\n",
    "print(\"Pass 2: transform hidden states and store results\")\n",
    "pca_results = []\n",
    "for i in range(len(zarr_dataset)):\n",
    "    item = zarr_dataset[i]\n",
    "    label_idx = item[\"label_tensor\"].item()\n",
    "\n",
    "    # Skip samples labeled as NONE\n",
    "    if label_idx == cfg.data.emotion_idx[\"NONE\"]:\n",
    "        continue\n",
    "\n",
    "    data_tensor = item[\"data_tensor\"].unsqueeze(0).float().to(device)\n",
    "    with torch.no_grad():\n",
    "        _, hidden = model(data_tensor, return_hidden=True)\n",
    "\n",
    "    hidden_np = hidden.cpu().numpy()\n",
    "    pc = ipca.transform(hidden_np)[0]  # shape (1, n_components)\n",
    "    pc1, pc2 = pc[0], pc[1] if cfg.pca.n_components >= 2 else (pc[0], None)\n",
    "\n",
    "    emotion_str = inverse_emotion_idx.get(label_idx, \"UNKNOWN\")\n",
    "    time_offset = item[\"time_offset\"]\n",
    "    subject = item[\"subject\"]\n",
    "\n",
    "    pca_results.append([pc1, pc2, emotion_str, time_offset, subject])\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    pca_results,\n",
    "    columns=[\"PC1\", \"PC2\", \"EmotionLabel\", \"time_offset\", \"subject\"]\n",
    ")\n",
    "\n",
    "# 7) Sort results: first by subject, then by time_offset\n",
    "df_sorted = df.sort_values([\"subject\", \"time_offset\"]).reset_index(drop=True)\n",
    "\n",
    "# 8) Save to CSV\n",
    "os.makedirs(cfg.output.csv_dir, exist_ok=True)\n",
    "out_csv_path = os.path.join(cfg.output.csv_dir, cfg.output.csv_name)\n",
    "df_sorted.to_csv(out_csv_path, index=False)\n",
    "print(f\"PCA results saved to {out_csv_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepemotion-r8YRC923-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
