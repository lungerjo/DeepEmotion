{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT: /home/paperspace/DeepEmotion\n",
      "/home/paperspace/DeepEmotion/output/models/sub_ALL.pth\n",
      "Loaded model from /home/paperspace/DeepEmotion/output/models/sub_ALL.pth\n",
      "Time 6760.0s, volume_idx=133, row_idx=119 Subject=sub-01, Session=01 | True=SADNESS, Predicted=SADNESS\n",
      "Time 6760.0s, volume_idx=134, row_idx=119 Subject=sub-02, Session=02 | True=SADNESS, Predicted=SADNESS\n",
      "Time 6760.0s, volume_idx=135, row_idx=119 Subject=sub-03, Session=03 | True=SADNESS, Predicted=SADNESS\n",
      "Time 6760.0s, volume_idx=136, row_idx=119 Subject=sub-04, Session=04 | True=SADNESS, Predicted=LOVE\n",
      "Time 6760.0s, volume_idx=137, row_idx=119 Subject=sub-05, Session=05 | True=SADNESS, Predicted=SADNESS\n",
      "Time 6760.0s, volume_idx=138, row_idx=119 Subject=sub-06, Session=06 | True=SADNESS, Predicted=SADNESS\n",
      "Time 6760.0s, volume_idx=139, row_idx=119 Subject=sub-07, Session=07 | True=SADNESS, Predicted=LOVE\n",
      "Time 6760.0s, volume_idx=140, row_idx=119 Subject=sub-08, Session=08 | True=SADNESS, Predicted=SADNESS\n",
      "Time 6760.0s, volume_idx=141, row_idx=119 Subject=sub-09, Session=08 | True=SADNESS, Predicted=SADNESS\n",
      "Time 6760.0s, volume_idx=142, row_idx=119 Subject=sub-11, Session=08 | True=SADNESS, Predicted=SADNESS\n",
      "Time 6760.0s, volume_idx=143, row_idx=119 Subject=sub-12, Session=08 | True=SADNESS, Predicted=SADNESS\n",
      "Time 6760.0s, volume_idx=144, row_idx=119 Subject=sub-13, Session=08 | True=SADNESS, Predicted=SADNESS\n",
      "Time 6760.0s, volume_idx=145, row_idx=119 Subject=sub-14, Session=08 | True=SADNESS, Predicted=SADNESS\n",
      "Time 6760.0s, volume_idx=146, row_idx=119 Subject=sub-15, Session=08 | True=SADNESS, Predicted=SADNESS\n",
      "Time 6760.0s, volume_idx=147, row_idx=119 Subject=sub-16, Session=08 | True=SADNESS, Predicted=SADNESS\n",
      "Time 6760.0s, volume_idx=148, row_idx=119 Subject=sub-17, Session=08 | True=SADNESS, Predicted=SADNESS\n",
      "Time 6760.0s, volume_idx=149, row_idx=119 Subject=sub-18, Session=08 | True=SADNESS, Predicted=SADNESS\n",
      "Time 6760.0s, volume_idx=150, row_idx=119 Subject=sub-19, Session=08 | True=SADNESS, Predicted=SADNESS\n",
      "Time 6760.0s, volume_idx=151, row_idx=119 Subject=sub-20, Session=08 | True=SADNESS, Predicted=SADNESS\n"
     ]
    }
   ],
   "source": [
    "# In[ ]:\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "from utils.dataset import ZarrDataset\n",
    "from models.CNN import CNN\n",
    "from models.resnet import ResNet, BasicBlock\n",
    "\n",
    "def build_offset_map(zarr_dataset):\n",
    "    \"\"\"\n",
    "    Creates a dictionary of offset -> list of (volume_idx, row_idx) for quick lookup.\n",
    "    \"\"\"\n",
    "    offset_to_indices = {}\n",
    "    if zarr_dataset.aligned_labels is None:\n",
    "        print(\"No aligned_labels found in ZarrDataset. Make sure 'aligned_labels' attribute is present.\")\n",
    "        return offset_to_indices\n",
    "\n",
    "    # For each row in aligned_labels, gather (volume_idx, row_index).\n",
    "    for i, row in zarr_dataset.aligned_labels.iterrows():\n",
    "        offset = row['time_offset']\n",
    "        vol_idx = row['file_index']\n",
    "        t_idx = row['row_index']\n",
    "        offset_to_indices.setdefault(offset, []).append((vol_idx, t_idx))\n",
    "\n",
    "    return offset_to_indices\n",
    "\n",
    "def forward_model_on_offset(model, zarr_dataset, offset, offset_map, device):\n",
    "    \"\"\"\n",
    "    Find all samples in zarr_dataset for a given time offset, run them through the model,\n",
    "    and return a list of predictions and any metadata you want to track.\n",
    "    \"\"\"\n",
    "    # Find all (volume_idx, row_idx) pairs for this offset\n",
    "    if offset not in offset_map:\n",
    "        print(f\"No samples found for offset {offset}\")\n",
    "        return []\n",
    "\n",
    "    results = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for (vol_idx, row_idx) in offset_map[offset]:\n",
    "            # We must find the dataset index (the \"flattened\" index) that corresponds to (vol_idx, row_idx).\n",
    "            # Because ZarrDataset.__getitem__ uses self.valid_indices, we look up the pair in valid_indices:\n",
    "            try:\n",
    "                dataset_index = zarr_dataset.valid_indices.index((vol_idx, row_idx))\n",
    "            except ValueError:\n",
    "                # This means (vol_idx, row_idx) wasn't in valid_indices (it might be an unlabeled sample)\n",
    "                continue\n",
    "            \n",
    "            item = zarr_dataset[dataset_index]\n",
    "            data_tensor = item[\"data_tensor\"].unsqueeze(0).float().to(device)\n",
    "            if data_tensor.dim() == 4:\n",
    "                # Model expects (batch, channels, x, y, z)\n",
    "                data_tensor = data_tensor.unsqueeze(1)\n",
    "\n",
    "            output = model(data_tensor)\n",
    "            _, predicted_idx = torch.max(output, dim=1)\n",
    "            predicted_idx = predicted_idx.item()\n",
    "\n",
    "            results.append({\n",
    "                \"volume_idx\": vol_idx,\n",
    "                \"row_idx\": row_idx,\n",
    "                \"time_offset\": item[\"time_offset\"],\n",
    "                \"true_label_idx\": item[\"label_tensor\"].item(),\n",
    "                \"predicted_idx\": predicted_idx,\n",
    "                \"subject\": item[\"subject\"],\n",
    "                \"session\": item[\"session\"]\n",
    "            })\n",
    "    return results\n",
    "\n",
    "# -------------------- Example usage --------------------\n",
    "\n",
    "# 1) Build an in-memory config (or load your real Hydra config)\n",
    "PROJECT_ROOT = os.path.abspath(\"../../\")\n",
    "print(f\"PROJECT_ROOT: {PROJECT_ROOT}\")\n",
    "cfg = OmegaConf.create({\n",
    "    \"project_root\": PROJECT_ROOT,\n",
    "    \"verbose\": True,\n",
    "    \"wandb\": True,\n",
    "    \"sys_log\": True,\n",
    "    \"model\": \"CNN\",\n",
    "    \"CNN\": {\n",
    "        \"c1\": 16, \"c2\": 32, \"c3\": 64, \"k1\": 3, \"k2\": 3, \"k3\": 3,\n",
    "        \"pk\": 2, \"ps\": 2, \"kernel_size\": 3, \"stride\": 1, \"padding\": 1\n",
    "    },\n",
    "    \"train\": {\n",
    "        \"epochs\": 50, \"batch_size\": 20, \"shuffle\": True, \"train_ratio\": 0.8,\n",
    "        \"print_label_frequencies\": True\n",
    "    },\n",
    "    \"data\": {\n",
    "        \"data_path\": f\"{PROJECT_ROOT}/data/raw/derivatives/non-linear_anatomical_alignment\",\n",
    "        \"zarr_dir_path\": f\"{PROJECT_ROOT}/zarr_datasets\",\n",
    "        \"zarr_path\": f\"{PROJECT_ROOT}/zarr_datasets/pool_emotions\",\n",
    "        \"label_path\": f\"{PROJECT_ROOT}/data/updated_annotations/pooled_annotations_structured.tsv\",\n",
    "        \"sessions\": [\"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\"],\n",
    "        \"file_pattern_template\": \"*_ses-forrestgump_task-forrestgump_rec-dico7Tad2grpbold7TadNL_run-{}_bold.nii.gz\",\n",
    "        \"subjects\": [\"sub-20\"],\n",
    "        \"session_offsets\": [0, 902, 1784, 2660, 3636, 4560, 5438, 6522],\n",
    "        \"emotion_idx\": {\"NONE\": 0, \"HAPPINESS\": 1, \"FEAR\": 2, \"SADNESS\": 3, \"LOVE\": 4, \"ANGER\": 5},\n",
    "        \"normalization\": False,\n",
    "        \"weight_decay\": 0,\n",
    "        \"learning_rate\": 0.0001,\n",
    "        \"seed\": 42,\n",
    "        \"save_model\": True,\n",
    "        \"load_model\": True,\n",
    "        \"save_model_path\": \"output/models\",\n",
    "        \"load_model_path\": f\"{PROJECT_ROOT}/output/models/sub_ALL.pth\",\n",
    "        \"output_csv_path\": f\"{PROJECT_ROOT}/output/inference/sub_ALL.csv\"\n",
    "    }\n",
    "})\n",
    "\n",
    "print(cfg.data.load_model_path)\n",
    "\n",
    "# 2) Load dataset\n",
    "zarr_dataset = ZarrDataset(cfg.data.zarr_path)\n",
    "\n",
    "# 3) Build the offset map for quick lookups\n",
    "offset_map = build_offset_map(zarr_dataset)\n",
    "\n",
    "# 4) Instantiate the model and load weights\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "emotion_idx = cfg.data.emotion_idx\n",
    "output_dim = len(emotion_idx)\n",
    "\n",
    "if cfg.model == \"CNN\":\n",
    "    from models.CNN import CNN\n",
    "    model = CNN(cfg=cfg, output_dim=output_dim)\n",
    "elif cfg.model == \"ResNet\":\n",
    "    from models.resnet import ResNet, BasicBlock\n",
    "    model = ResNet(BasicBlock, [1, 1, 1, 1], in_channels=1, num_classes=output_dim)\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported model: {cfg.model}\")\n",
    "\n",
    "model_path = cfg.data.load_model_path\n",
    "if os.path.exists(model_path):\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    print(f\"Loaded model from {model_path}\")\n",
    "model.to(device)\n",
    "\n",
    "# 5) Run inference for a chosen offset\n",
    "sample_offset = 6760.0  # seconds into the movie, for instance\n",
    "results = forward_model_on_offset(model, zarr_dataset, sample_offset, offset_map, device)\n",
    "\n",
    "# 6) Map prediction indices back to emotion names\n",
    "inv_idx = {v: k for k, v in emotion_idx.items()}\n",
    "\n",
    "for r in results:\n",
    "    pred_name = inv_idx[r[\"predicted_idx\"]]\n",
    "    true_name = inv_idx[r[\"true_label_idx\"]]\n",
    "    print(f\"Time {r['time_offset']:.1f}s, volume_idx={r['volume_idx']}, row_idx={r['row_idx']} \"\n",
    "          f\"Subject={r['subject']}, Session={r['session']} | \"\n",
    "          f\"True={true_name}, Predicted={pred_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples (including NONE): 16682\n",
      "Loaded model weights from /home/paperspace/DeepEmotion/output/models/sub_ALL.pth\n",
      "Pass 1: perform PCA\n",
      "Pass 2: transform hidden states and store results\n",
      "PCA results saved to /home/paperspace/DeepEmotion/output/inference/sub_ALL_ao.csv\n"
     ]
    }
   ],
   "source": [
    "# In[ ]:\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "from utils.dataset import ZarrDataset\n",
    "from models.CNN import CNN\n",
    "from models.resnet import ResNet, BasicBlock\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "###############################################################################\n",
    "# Example: Perform incremental PCA on the full dataset by directly iterating \n",
    "# over ZarrDataset. Produces a CSV with [PC1, PC2, EmotionLabel, time_offset, subject],\n",
    "# sorted first by subject, then by time offset.\n",
    "###############################################################################\n",
    "\n",
    "# 1) Define or load a Hydra config\n",
    "cfg = OmegaConf.create({\n",
    "    \"project_root\": PROJECT_ROOT,\n",
    "    \"verbose\": True,\n",
    "    \"wandb\": True,\n",
    "    \"sys_log\": True,\n",
    "    \"model\": \"CNN\",\n",
    "    \"CNN\": {\n",
    "        \"c1\": 16, \"c2\": 32, \"c3\": 64, \"k1\": 3, \"k2\": 3, \"k3\": 3,\n",
    "        \"pk\": 2, \"ps\": 2, \"kernel_size\": 3, \"stride\": 1, \"padding\": 1\n",
    "    },\n",
    "    \"train\": {\n",
    "        \"epochs\": 50, \"batch_size\": 20, \"shuffle\": True, \"train_ratio\": 0.8,\n",
    "        \"print_label_frequencies\": True\n",
    "    },\n",
    "    \"data\": {\n",
    "        \"data_path\": f\"{PROJECT_ROOT}/data/raw/derivatives/non-linear_anatomical_alignment\",\n",
    "        \"zarr_dir_path\": f\"{PROJECT_ROOT}/zarr_datasets\",\n",
    "        \"zarr_path\": f\"{PROJECT_ROOT}/zarr_datasets/pool_emotions\",\n",
    "        \"label_path\": f\"{PROJECT_ROOT}/data/updated_annotations/pooled_annotations_structured.tsv\",\n",
    "        \"sessions\": [\"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\"],\n",
    "        \"file_pattern_template\": \"*_ses-forrestgump_task-forrestgump_rec-dico7Tad2grpbold7TadNL_run-{}_bold.nii.gz\",\n",
    "        \"subjects\": [\"sub-20\"],\n",
    "        \"session_offsets\": [0, 902, 1784, 2660, 3636, 4560, 5438, 6522],\n",
    "        \"emotion_idx\": {\"NONE\": 0, \"HAPPINESS\": 1, \"FEAR\": 2, \"SADNESS\": 3, \"LOVE\": 4, \"ANGER\": 5},\n",
    "        \"normalization\": False,\n",
    "        \"weight_decay\": 0,\n",
    "        \"learning_rate\": 0.0001,\n",
    "        \"seed\": 42,\n",
    "        \"save_model\": True,\n",
    "        \"load_model\": True,\n",
    "        \"save_model_path\": \"output/models\",\n",
    "        \"load_model_path\": f\"{PROJECT_ROOT}/output/models/sub_ALL.pth\",\n",
    "        \"output_csv_path\": f\"{PROJECT_ROOT}/output/inference/sub_ALL_ao.csv\"\n",
    "    }\n",
    "})\n",
    "\n",
    "# 2) Load your ZarrDataset\n",
    "zarr_dataset = ZarrDataset(cfg.data.zarr_path)\n",
    "print(f\"Total samples (including NONE): {len(zarr_dataset)}\")\n",
    "\n",
    "# 3) Build an inverse emotion mapping for easier label naming\n",
    "inverse_emotion_idx = {v: k for k, v in cfg.data.emotion_idx.items()}\n",
    "\n",
    "# 4) Instantiate the model and load weights\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if cfg.model == \"CNN\":\n",
    "    model = CNN(cfg=cfg, output_dim=len(cfg.data.emotion_idx))\n",
    "elif cfg.model == \"ResNet\":\n",
    "    model = ResNet(BasicBlock, [1,1,1,1], in_channels=1, num_classes=len(cfg.data.emotion_idx))\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported model: {cfg.model}\")\n",
    "\n",
    "if os.path.exists(cfg.data.load_model_path):\n",
    "    model.load_state_dict(torch.load(cfg.data.load_model_path, map_location=device))\n",
    "    print(f\"Loaded model weights from {cfg.data.load_model_path}\")\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# 5) Incremental PCA: partial_fit in one pass\n",
    "ipca = IncrementalPCA(n_components=2)\n",
    "buffer = []\n",
    "\n",
    "print(\"Pass 1: perform PCA\")\n",
    "for i in range(len(zarr_dataset)):\n",
    "    item = zarr_dataset[i]\n",
    "    label_idx = item[\"label_tensor\"].item()\n",
    "\n",
    "    if label_idx == cfg.data.emotion_idx[\"NONE\"]:\n",
    "        continue\n",
    "\n",
    "    data_tensor = item[\"data_tensor\"].unsqueeze(0).float().to(device)\n",
    "    with torch.no_grad():\n",
    "        _, hidden = model(data_tensor, return_hidden=True)\n",
    "    hidden_np = hidden.cpu().numpy()\n",
    "\n",
    "    buffer.append(hidden_np)\n",
    "\n",
    "    if len(buffer) == 10:  # buffer size >= n_components\n",
    "        stacked = np.vstack(buffer)\n",
    "        ipca.partial_fit(stacked)\n",
    "        buffer.clear()\n",
    "\n",
    "# Optional: partial_fit remaining samples\n",
    "if buffer:\n",
    "    stacked = np.vstack(buffer)\n",
    "    ipca.partial_fit(stacked)\n",
    "\n",
    "# 6) Second pass: transform, collect (PC1, PC2) + [Emotion, time_offset, subject]\n",
    "print(\"Pass 2: transform hidden states and store results\")\n",
    "pca_results = []\n",
    "for i in range(len(zarr_dataset)):\n",
    "    item = zarr_dataset[i]\n",
    "    label_idx = item[\"label_tensor\"].item()\n",
    "\n",
    "    # Skip samples labeled as NONE\n",
    "    if label_idx == cfg.data.emotion_idx[\"NONE\"]:\n",
    "        continue\n",
    "\n",
    "    data_tensor = item[\"data_tensor\"].unsqueeze(0).float().to(device)\n",
    "    with torch.no_grad():\n",
    "        _, hidden = model(data_tensor, return_hidden=True)\n",
    "\n",
    "    hidden_np = hidden.cpu().numpy()\n",
    "    pc = ipca.transform(hidden_np)[0]  # shape (1, n_components)\n",
    "    pc1, pc2 = pc[0], pc[1]\n",
    "\n",
    "    emotion_str = inverse_emotion_idx.get(label_idx, \"UNKNOWN\")\n",
    "    time_offset = item[\"time_offset\"]\n",
    "    subject = item[\"subject\"]\n",
    "\n",
    "    pca_results.append([pc1, pc2, emotion_str, time_offset, subject])\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    pca_results,\n",
    "    columns=[\"PC1\", \"PC2\", \"EmotionLabel\", \"time_offset\", \"subject\"]\n",
    ")\n",
    "\n",
    "# 7) Sort results: first by subject, then by time_offset\n",
    "df_sorted = df.sort_values([\"subject\", \"time_offset\"]).reset_index(drop=True)\n",
    "\n",
    "# 8) Save to CSV\n",
    "df_sorted.to_csv(cfg.data.output_csv_path, index=False)\n",
    "print(f\"PCA results saved to {cfg.data.output_csv_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepemotion-r8YRC923-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
